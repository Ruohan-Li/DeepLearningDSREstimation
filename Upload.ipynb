{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64497e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num GPUs Available:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "CUDA_VISIBLE_DEVICES=1\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError as rmse\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras import backend\n",
    "from keras.layers import Dropout, Input, BatchNormalization, concatenate, Activation, MaxPooling2D, AveragePooling2D, Flatten, Masking\n",
    "from keras.layers import Bidirectional, GRU, Conv2D, TimeDistributed, LSTM, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true)))\n",
    "\n",
    "def BIAS(y_true, y_pred):\n",
    "\treturn backend.mean((y_pred - y_true), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986776ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 00:28:45.854057: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 632us/step - loss: 268655.5000 - RMSE: 515.7129 - BIAS: -425.1102\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 0s 590us/step - loss: 176332.6719 - RMSE: 414.0849 - BIAS: -312.4889\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 0s 627us/step - loss: 54175.3984 - RMSE: 227.1171 - BIAS: -66.9613\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 0s 612us/step - loss: 30429.7949 - RMSE: 171.5508 - BIAS: 3.5552\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 0s 611us/step - loss: 22413.1660 - RMSE: 145.6960 - BIAS: 2.4099\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 0s 609us/step - loss: 17221.2246 - RMSE: 126.8211 - BIAS: 0.0605\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 0s 602us/step - loss: 14926.2285 - RMSE: 117.5199 - BIAS: -2.3996\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 0s 617us/step - loss: 13653.4785 - RMSE: 112.5108 - BIAS: -2.1635\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 0s 608us/step - loss: 12993.7480 - RMSE: 110.4213 - BIAS: -1.8388\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 0s 603us/step - loss: 12531.2656 - RMSE: 108.3732 - BIAS: -0.9775\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 0s 588us/step - loss: 12274.0342 - RMSE: 107.2122 - BIAS: -1.1563\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 0s 602us/step - loss: 12046.5059 - RMSE: 106.0927 - BIAS: -0.1808\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 0s 619us/step - loss: 11878.3164 - RMSE: 105.6666 - BIAS: -0.6719\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 0s 607us/step - loss: 11758.3857 - RMSE: 104.9563 - BIAS: -0.0317\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 0s 588us/step - loss: 11651.7285 - RMSE: 104.1528 - BIAS: -0.7844\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 0s 577us/step - loss: 11567.9541 - RMSE: 103.9056 - BIAS: -0.5093\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 0s 620us/step - loss: 11469.7920 - RMSE: 103.9131 - BIAS: -0.6713\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 0s 531us/step - loss: 11435.7334 - RMSE: 103.0853 - BIAS: -0.2503\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 0s 515us/step - loss: 11365.6367 - RMSE: 103.5272 - BIAS: -0.5763\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 0s 535us/step - loss: 11312.9307 - RMSE: 103.0217 - BIAS: 0.3024\n"
     ]
    }
   ],
   "source": [
    "data = np.load('mlp_data.npz',allow_pickle=True)\n",
    "mlp_X,mlp_Y = data['arr_0'], data['arr_1']\n",
    "\n",
    "def build_model_mlp(trainX):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(25,activation='relu',input_shape=(trainX.shape[1],)))\n",
    "    model.add(Dense(5,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='rmsprop',loss='mse',metrics=[RMSE,BIAS])\n",
    "    return model\n",
    "\n",
    "model=build_model_mlp(mlp_X)\n",
    "history=model.fit(mlp_X,mlp_Y,epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51b38bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lir/.conda/envs/lenv2/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 24ms/step - loss: 201796.1094 - RMSE: 450.5330 - BIAS: -326.9475\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 200954.5000 - RMSE: 444.2608 - BIAS: -325.6695\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 199522.3125 - RMSE: 448.9027 - BIAS: -323.8145\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 197283.7500 - RMSE: 441.4278 - BIAS: -320.9561\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 193914.0469 - RMSE: 443.6653 - BIAS: -316.1460\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 189405.9844 - RMSE: 435.5395 - BIAS: -308.9622\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 184060.4531 - RMSE: 432.3969 - BIAS: -300.2178\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 178463.4062 - RMSE: 422.3336 - BIAS: -290.8792\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 172587.4844 - RMSE: 414.2118 - BIAS: -281.1919\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 166249.3281 - RMSE: 406.2711 - BIAS: -270.6034\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 159447.5781 - RMSE: 397.8429 - BIAS: -259.0859\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 152424.1250 - RMSE: 393.3512 - BIAS: -246.2561\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 144891.1094 - RMSE: 373.8281 - BIAS: -232.7207\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 137138.2344 - RMSE: 372.3756 - BIAS: -218.5253\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 129052.1484 - RMSE: 363.8440 - BIAS: -203.3782\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 121243.9219 - RMSE: 342.8207 - BIAS: -187.2660\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 112937.5312 - RMSE: 333.7179 - BIAS: -172.6087\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 105048.8047 - RMSE: 320.6018 - BIAS: -158.8385\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 97482.8828 - RMSE: 309.4296 - BIAS: -144.8559\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 90225.2891 - RMSE: 294.5138 - BIAS: -130.5847\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 83259.6484 - RMSE: 287.2776 - BIAS: -116.7602\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 76441.7188 - RMSE: 274.0876 - BIAS: -106.0154\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 69493.0234 - RMSE: 263.5068 - BIAS: -95.8387\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 63154.8125 - RMSE: 248.1507 - BIAS: -87.1597\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 56674.2852 - RMSE: 238.0304 - BIAS: -84.1987\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 50945.1250 - RMSE: 227.8609 - BIAS: -71.1011\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 45362.2305 - RMSE: 208.4265 - BIAS: -70.5347\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 40372.8359 - RMSE: 200.6098 - BIAS: -59.0077\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 36354.7578 - RMSE: 186.1855 - BIAS: -55.0539\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 32369.6562 - RMSE: 178.6811 - BIAS: -53.0865\n"
     ]
    }
   ],
   "source": [
    "data = np.load('bigru_data.npz',allow_pickle=True)\n",
    "bigru_X,bigru_Y = data['arr_0'], data['arr_1']\n",
    "\n",
    "def build_model_bigru(trainX):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(32,  input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True)))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=5, activation='relu'))\n",
    "    model.add(Dense(units=1))\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss='mse',metrics=[RMSE,BIAS])\n",
    "    return model\n",
    "\n",
    "model=build_model_bigru(bigru_X)\n",
    "history=model.fit(bigru_X,bigru_Y, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbb64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lir/.conda/envs/lenv2/lib/python3.9/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 7s 17ms/step - loss: 234186.3438 - RMSE: 479.7634 - BIAS: -408.2061\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 108363.2422 - RMSE: 320.9772 - BIAS: -274.7201\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 25192.1641 - RMSE: 151.1797 - BIAS: -83.9304\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 14184.8193 - RMSE: 115.5281 - BIAS: -6.1809\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 12859.2861 - RMSE: 110.1476 - BIAS: -1.9900\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 12815.0381 - RMSE: 109.7067 - BIAS: -1.3856\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 12559.3398 - RMSE: 108.4762 - BIAS: -1.4636\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 11815.2305 - RMSE: 105.1498 - BIAS: -1.2818\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 11777.6191 - RMSE: 104.7096 - BIAS: -1.4333\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 11659.3096 - RMSE: 104.9777 - BIAS: -1.2510\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 11068.7588 - RMSE: 101.7134 - BIAS: -0.3971\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 10794.8799 - RMSE: 100.8170 - BIAS: -1.3884\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 10586.5771 - RMSE: 99.8421 - BIAS: -0.8563\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 10396.9443 - RMSE: 99.1323 - BIAS: -0.6706\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 10236.0762 - RMSE: 98.1028 - BIAS: -0.7186\n"
     ]
    }
   ],
   "source": [
    "data = np.load('densenet_data.npz',allow_pickle=True)\n",
    "densenet_X,densenet_Y = data['arr_0'], data['arr_1']\n",
    "\n",
    "def conv_layer(conv_x, filters):\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n",
    "    conv_x = Dropout(0.2)(conv_x)\n",
    "\n",
    "    return conv_x\n",
    "\n",
    "\n",
    "def dense_block(block_x, filters, growth_rate, layers_in_block):\n",
    "    for i in range(layers_in_block):\n",
    "        each_layer = conv_layer(block_x, growth_rate)\n",
    "        block_x = concatenate([block_x, each_layer], axis=-1)\n",
    "        filters += growth_rate\n",
    "\n",
    "    return block_x, filters\n",
    "\n",
    "\n",
    "def transition_block(trans_x, tran_filters):\n",
    "    trans_x = BatchNormalization()(trans_x)\n",
    "    trans_x = Activation('relu')(trans_x)\n",
    "    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n",
    "    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)\n",
    "\n",
    "    return trans_x, tran_filters\n",
    "\n",
    "\n",
    "def dense_net(in_shape,out_shape,filters, growth_rate, dense_block_size, layers_in_block):\n",
    "    input_img = Input(shape=in_shape)\n",
    "    x = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)\n",
    "\n",
    "    dense_x = BatchNormalization()(x)\n",
    "    dense_x = Activation('relu')(x)\n",
    "\n",
    "    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(dense_x)\n",
    "    for block in range(dense_block_size - 1):\n",
    "        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "        dense_x, filters = transition_block(dense_x, filters)\n",
    "\n",
    "    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "    dense_x = BatchNormalization()(dense_x)\n",
    "    dense_x = Activation('relu')(dense_x)\n",
    "    dense_x = Flatten()(dense_x)\n",
    "\n",
    "    output = Dense(out_shape, activation='relu')(dense_x)\n",
    "\n",
    "    return Model(input_img, output)\n",
    "\n",
    "dense_block_size = 3\n",
    "layers_in_block = 3\n",
    "growth_rate = 4\n",
    "model_merge = dense_net((densenet_X.shape[1],densenet_X.shape[2],densenet_X.shape[3]),1,growth_rate * 2, growth_rate, dense_block_size, layers_in_block)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
    "model_merge.compile(optimizer = optimizer, loss='mse',metrics=[RMSE,BIAS])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "history=model_merge.fit(densenet_X,densenet_Y, epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e5550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38/38 [==============================] - 5s 82ms/step - loss: 197526.3906 - RMSE: 439.6683 - BIAS: -321.5135\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 194053.7812 - RMSE: 437.2147 - BIAS: -316.6888\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 191836.5625 - RMSE: 436.2387 - BIAS: -314.4688\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 189521.7656 - RMSE: 431.1381 - BIAS: -311.8939\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 187043.6562 - RMSE: 429.9130 - BIAS: -308.9781\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 184420.8750 - RMSE: 427.2227 - BIAS: -305.7776\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 181634.2812 - RMSE: 420.5208 - BIAS: -302.3031\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 178706.3125 - RMSE: 418.8466 - BIAS: -298.5723\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 175606.6562 - RMSE: 413.3956 - BIAS: -294.6037\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 172350.9531 - RMSE: 411.1624 - BIAS: -290.4523\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 169009.1406 - RMSE: 407.6501 - BIAS: -285.9362\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 165493.6719 - RMSE: 402.8920 - BIAS: -281.2721\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 161855.4844 - RMSE: 397.7289 - BIAS: -276.4907\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 158166.3125 - RMSE: 395.4215 - BIAS: -271.2803\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 154377.7031 - RMSE: 389.7544 - BIAS: -266.2601\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 150485.4688 - RMSE: 384.7114 - BIAS: -260.9030\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 146532.5469 - RMSE: 380.1338 - BIAS: -255.9163\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 142556.8125 - RMSE: 375.8907 - BIAS: -250.7728\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 138480.4844 - RMSE: 370.1484 - BIAS: -245.4393\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 134416.0625 - RMSE: 362.2784 - BIAS: -239.8041\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 130346.2969 - RMSE: 356.8846 - BIAS: -234.2320\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 126339.8438 - RMSE: 351.5020 - BIAS: -228.6505\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 122269.6016 - RMSE: 344.7451 - BIAS: -222.7969\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 118231.4297 - RMSE: 339.4397 - BIAS: -217.3122\n",
      "Epoch 25/30\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 114248.5078 - RMSE: 334.2939 - BIAS: -211.3790\n",
      "Epoch 26/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 110270.4844 - RMSE: 329.6485 - BIAS: -205.9438\n",
      "Epoch 27/30\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 106339.5781 - RMSE: 321.4435 - BIAS: -200.1656\n",
      "Epoch 28/30\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 102498.1797 - RMSE: 315.3896 - BIAS: -194.5878\n",
      "Epoch 29/30\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 98782.7969 - RMSE: 310.4411 - BIAS: -189.2480\n",
      "Epoch 30/30\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 94972.0703 - RMSE: 306.8187 - BIAS: -183.6154\n"
     ]
    }
   ],
   "source": [
    "data = np.load('cnngru_data.npz',allow_pickle=True)\n",
    "cnngru_X,cnngru_Y = data['arr_0'], data['arr_1']\n",
    "\n",
    "def build_model_cnngru(trainX):\n",
    "    shape = (trainX.shape[1], trainX.shape[2], trainX.shape[3], trainX.shape[4])\n",
    "    nLstmNeu=32\n",
    "\n",
    "    model = Sequential()\n",
    "    # --- CNN\n",
    "    model.add(TimeDistributed(Conv2D(filters=16,kernel_size=(3,3),padding='same',kernel_regularizer=regularizers.l2(l=0.01),\n",
    "                                     activation='relu',input_shape=shape)))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(filters=32,kernel_size=(3,3),padding='same',kernel_regularizer=regularizers.l2(l=0.01),\n",
    "                                     activation='relu')))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(filters=64,kernel_size=(3,3),padding='same',kernel_regularizer=regularizers.l2(l=0.1),\n",
    "                                      activation='relu')))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    #model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "    # --- LSTM\n",
    "    model.add(Bidirectional(GRU(nLstmNeu, return_sequences=True, activation='tanh')))\n",
    "    model.add(TimeDistributed(Dense(8, activation='relu',\n",
    "                                    kernel_initializer=\"glorot_uniform\",kernel_regularizer=regularizers.l2(l=0.01))))\n",
    "    model.add(TimeDistributed(Dense(1, activation='relu',\n",
    "                                    kernel_initializer=\"glorot_uniform\",kernel_regularizer=regularizers.l2(l=0.01))))\n",
    "    # --- Optimizer \n",
    "    adam = keras.optimizers.Adam(lr=0.0005)\n",
    "    model.compile(optimizer=adam, loss='mse',metrics=[RMSE,BIAS])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=build_model_cnngru(cnngru_X)\n",
    "history=model.fit(cnngru_X,cnngru_Y, batch_size=8,epochs=30)#10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49a4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('cnngru_model.h5',custom_objects={'RMSE':RMSE,'BIAS':BIAS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def9625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
